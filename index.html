<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Bo Yang | University of Oxford</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/oxford_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Bo Yang</name></p>
                  <p align="justify">I am an incoming Assistant Professor (2020.11-) in the <a href="https://www.comp.polyu.edu.hk/">Department of Computing</a>
                    at <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>. I completed my D.Phil degree (2016.10-2020.09)
                    in the <a href="http://www.cs.ox.ac.uk/">Department of Computer Science</a>
                    at <a href="http://www.ox.ac.uk/">University of Oxford</a>,
                    supervised by Profs. <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/">Niki Trigoni</a>
                    and <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew Markham</a>.
                    Prior to Oxford, I obtained an M.Phil degree from <a href="https://www.hku.hk/">The University of Hong Kong</a>
                    where I was supervised by <a href="https://www.imse.hku.hk/people/s-h-choi">Prof. S.H. Choi</a>,
                    and a B.Eng degree from <a href="https://english.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>.

                    </br></br>
                    In my D.Phil study, I interned at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).
                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.
                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).

	            </br>
                </p><p align="center">
                    <a href="mailto:bo.yang[-at-]cs.ox.ac.uk">Email</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=VqUAqz8AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Yang7879"> Github </a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
           <p align="justify">I'm interested in machine learning, computer vision, and robotics. My research goal is to
               build intelligent systems which endow machines to recover, understand, and eventually interact with the real 3D world.
               This includes accurate and efficient recognition, segmentation and reconstruction of all individual objects within large-scale 3D scenes.
		   </p></br>
		   <font color="red"><strong>Research Positions:</strong> If you're interested in working with me, don't hesitate to drop an email. </font>
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News</heading>
            <p> <strong>[2020.09.10]</strong> Successfully defend my D.Phil thesis,
                examined by Profs. <a href="https://mpawankumar.info/">M. Pawan Kumar </a> and <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>.</p>
            <p> <strong>[2020.03.08]</strong> Invited to present our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                and <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>. </p>
            <p> <strong>[2020.02.27]</strong> One co-authored <a href="https://arxiv.org/abs/1911.11236">paper</a> for 3D semantic segmentation is accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. </p>
            <p> <strong>[2019.10.24]</strong> Successfully defend
                <a href="https://www.ox.ac.uk/students/academic/guidance/graduate/research/status/DPhil?wssl=1">D.Phil confirmation</a>, examined by Profs.
                <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Andrew Zisserman</a> and
                <a href="https://www.cs.ox.ac.uk/people/alessandro.abate/home.html">Alessandro Abate</a>.</p>
            <p> <strong>[2019.09.03]</strong> One first-authored paper for 3D instance segmentation is accepted as a spotlight at <a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a>. </p>
            <p> <strong>[2019.08.16]</strong> One first-authored paper for multi-view 3D reconstruction is accepted in <a href="https://link.springer.com/journal/11263">IJCV</a>. </p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading>Publications / Preprints</heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/20_arXiv_sensaturban.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://arxiv.org/abs/2009.03137">
                 <papertitle>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges</papertitle></a>
                 <br>Q. Hu, <strong>B. Yang*</strong>, S. Khalid, W. Xiao, N. Trigoni, A. Markham<br>
                 <a href="http://arxiv.org/abs/2009.03137"> arXiv, 2020</a>
                 <br>
                  <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                 <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <p align="justify" style="font-size:13px">We introduce an urban-scale photogrammetric point cloud dataset
                            and extensively evaluate and analyze the state-of-the-art algorithms on the dataset.
                  <br>(* indicates corresponding author)</p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/20_arXiv_pointloc.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2003.02392">
	            <papertitle>PointLoc: Deep Pose Regressor for LiDAR Point Cloud Localization</papertitle></a>
                <br>Wei Wang, Bing Wang, Peijun Zhao, Changhao Chen, Ronald Clark, <strong>B. Yang</strong>, Andrew Markham, Niki Trigoni
                <br>
                <a href="https://arxiv.org/abs/2003.02392">arXiv, 2020</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We present a learning-based LiDAR relocalization framework to efficiently estimate 6-DoF poses from LiDAR point clouds. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/20_cvpr_randlanet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1911.11236">
	            <papertitle>RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</papertitle></a>
                <br>Q. Hu, <strong>B. Yang*</strong>, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1911.11236">arXiv</a> /
                <a href="http://www.semantic3d.net/view_results.php">Semantic3D Benchmark</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/k_oROm1Zr6l0YNKGELx3Bw"><font color="red">(新智元,</font></a>
                <a href="https://mp.weixin.qq.com/s/Ed9v6I6l2tLTHmMW7B3O3g"><font color="red">AI科技评论,</font></a>
                <a href="https://mp.weixin.qq.com/s/TTv6pSPjmdsEF4kvVY-ZzQ"><font color="red">CVer)</font>/</a>
                <a href="https://www.youtube.com/watch?v=Ar3eY_lwzMk"><font color="red">Video</font></a>/
                <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>(* indicates corresponding author)
                </p><p></p>
			    <p align="justify" style="font-size:13px">We introduce an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. </p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/19_neurips_3d_bonet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/1906.01140">
	             <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</papertitle></a>
                 <br><strong>B. Yang</strong>, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, N. Trigoni
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="red"><strong>(Spotlight, 200/6743)</strong></font>
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/1906.01140">arXiv</a> /
                 <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> /
                 <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> /
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font color="red">(新智元,</font></a>
                 <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a>
                 <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font color="red">泡泡机器人)</font>/</a>
                 <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao"><font color="red">Video</font></a>/
                 <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds.
                  It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/19_iros_deeppco.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1910.11088">
	            <papertitle>DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network</papertitle></a>
                <br>W. Wang,  M.R.U. Saputra, P. Zhao, P. Gusmao, <strong>B. Yang</strong>, C. Chen, A. Markham, N. Trigoni<br>
		        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2019 <br>
                <a href="https://arxiv.org/abs/1910.11088">arXiv</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We propose a novel end-to-end deep parallel neural network to estimate the 6-DOF poses using consecutive 3D point clouds.</p>
            </td>
        </tr>


	    <td width="20%"><img src="./imgs/19_ijcv_attsets.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">
	            <papertitle>Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction</papertitle></a>
                <br><strong>B. Yang</strong>, S. Wang, A. Markham, N. Trigoni<br>
                <em>International Journal of Computer Vision (IJCV)</em>, 2019 <font color="red"><strong>(IF=6.07)</strong></font><br>
		        <a href="https://arxiv.org/abs/1808.00758">arXiv</a>/
		        <a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">Springer Open Access</a>/
                <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=AttSets&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose an attentive aggregation module together
                    with a training algorithm for multi-view 3D object reconstruction.
                    It outperforms all existing poolings and recurrent neural networks.</p>
            </td>
        </tr>


	    <td width="20%"><img src="./imgs/19_cvprw_embeddings.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">
	            <papertitle>Learning Semantically Meaningful Embeddings Using Linear Constraints</papertitle></a>
                <br>S. Lin, <strong>B. Yang</strong>, R. Birke, R. Clark<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)</em>, 2019<br>
                <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">CVF Open Access</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We propose a simple embedding learning method that jointly optimises for an auto-encoding reconstruction task
                    and for estimating the corresponding attribute labels.</p>
            </td>
        </tr>


	    <td width="20%"><img src="./imgs/18_tpami_3d_recgan++.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1802.00411">
	           <papertitle>Dense 3D Object Reconstruction from a Single Depth View</papertitle></a>
               <br><strong>B. Yang</strong>, S. Rosa, A. Markham, N. Trigoni, H. Wen<br>
               <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2018 <font color="red"><strong>(IF=17.73)</strong></font><br>
               <a href="https://arxiv.org/abs/1802.00411">arXiv</a>/
               <a href="https://ieeexplore.ieee.org/abstract/document/8453803">IEEE Xplore</a>/
               <a href="https://github.com/Yang7879/3D-RecGAN-extended"><font color="red">Code</font></a>
               <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-RecGAN-extended&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a novel neural architecture to reconstruct the complete 3D structure of a given object
                   from a single arbitrary depth view using generative adversarial networks.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/18_ijcai_3d_physnet.gif" alt="this slowpoke moves" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1805.00328">
	           <papertitle>3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations</papertitle></a>
               <br>Z. Wang, S. Rosa, <strong>B. Yang</strong>, S. Wang, N. Trigoni, A. Markham<br>
               <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2018 <br>
               <a href="https://arxiv.org/abs/1805.00328">arXiv</a>/
               <a href="https://github.com/vividda/3D-PhysNet"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=vividda&repo=3D-PhysNet&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a neural framework to predict how a 3D object will deform
                   under an applied force using intuitive physics modelling.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/18_cvprw_3r_d.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w9/html/Yang_Learning_3D_Scene_CVPR_2018_paper.html">
	           <papertitle>Learning 3D Scene Semantics and Structure from a Single Depth Image</papertitle></a>
               <br><strong>B. Yang*</strong>, Z. Lai*, X. Lu, S. Lin, H. Wen, A. Markham, N. Trigoni<br>
               <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)</em>, 2018 <br>
               <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w9/html/Yang_Learning_3D_Scene_CVPR_2018_paper.html">CVF Open Access</a> /
               <a href="https://ieeexplore.ieee.org/abstract/document/8575531">IEEE Xplore</a>
               <br>(* indicates equal contribution)
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose an efficient and holistic pipeline to simultaneously learn
                   the semantics and structure of a scene from a single depth image.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/18_icra_defonet.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1804.05928">
	           <papertitle>Defo-Net: Learning Body Deformation Using Generative Adversarial Networks</papertitle></a>
               <br>Z. Wang, S. Rosa, L. Xie, <strong>B. Yang</strong>, S. Wang, N. Trigoni, A. Markham<br>
               <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2018 <br>
               <a href="https://arxiv.org/abs/1804.05928">arXiv</a> /
               <a href="https://www.youtube.com/watch?v=noG5DDX3coQ"><font color="red">Video</font></a>/
               <a href="https://ieeexplore.ieee.org/abstract/document/8462832">IEEE Xplore</a>/
               <a href="https://github.com/vividda/Defo-Net"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=vividda&repo=Defo-Net&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a novel generative adversarial network to predict
                   body deformations under external forces from a single RGB-D image.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/17_iccvw_3d_recgan.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1708.07969">
	           <papertitle>3D Object Reconstruction from a Single Depth View with Adversarial Learning</papertitle></a>
               <br><strong>B. Yang</strong>, H. Wen, S. Wang, R. Clark, A. Markham, N. Trigoni<br>
               <em>IEEE International Conference on Computer Vision Workshops (ICCV-W) </em>, 2017 <br>
               <a href="https://arxiv.org/abs/1708.07969">arXiv</a> /
               <a href="https://ieeexplore.ieee.org/abstract/document/8265295">IEEE Xplore</a>/
               <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650730434&idx=4&sn=4a03526f020f30cc65b52976bb56f352&scene=0"><font color="red"> News: 机器之心</font>/</a>
               <a href="https://github.com/Yang7879/3D-RecGAN"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-RecGAN&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a novel approach to reconstruct the complete 3D structure of a given
                   object from a single arbitrary depth view using generative adversarial networks.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/16_mswim_bcs.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://dl.acm.org/citation.cfm?id=2989132">
	           <papertitle>Updating Wireless Signal Map with Bayesian Compressive Sensing</papertitle></a>
               <br><strong>B. Yang</strong>, S. He, S-H G. Chan<br>
               <em>ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM) </em>, 2016 <br>
               <a href="https://dl.acm.org/citation.cfm?id=2989132">ACM DL</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose Compressive Signal Reconstruction (CSR), a novel learning system
                   employing Bayesian compressive sensing (BCS) for online signal map update.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/15_compind_3dscan.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166361515000913">
	           <papertitle>A mechanised 3D scanning method for item-level radio frequency identification of palletised products</papertitle></a>
               <br>S.H. Choi, <strong>B. Yang</strong>, H.H. Cheung<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/abs/pii/S0166361515000913">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a mechanised 3D scanning method for identification of
                   tagged products in large numbers to facilitate supply chain management.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/15_compind_item_rfid.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/pii/S0166361515000482">
	           <papertitle>Item-level RFID for Enhancement of Customer Shopping Experience in Apparel Retail</papertitle></a>
               <br>S.H. Choi, Y.X. Yang, <strong>B. Yang</strong>, H.H. Cheung<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/pii/S0166361515000482">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose an item-level RFID-enabled retail store management system
                   for relatively high-end apparel products to provide customers with more leisure, interaction for product information.</p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/15_compind_tag_proc.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/pii/S0166361515000147">
	           <papertitle>RFID Tag Data Processing in Manufacturing for Track-and-Trace Anti-counterfeiting</papertitle></a>
               <br>S.H. Choi, <strong>B. Yang</strong>, H.H. Cheung, Y.X. Yang<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/pii/S0166361515000147">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a track-and-trace anti-counterfeiting system,
                   and propose a tag data processing and synchronization algorithm to generate initial e-pedigrees for products.</p>
            </td>
        </tr>

        </tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Teaching</heading>
             <p> <strong>Hilary Term, 2019</strong>: &ensp;&ensp; <a href="https://www.cs.ox.ac.uk/teaching/courses/2018-2019/KRR/"> Knowledge Representation & Reasoning </a> (University of Oxford). </p>

             <p> <strong>Michaelmas Term, 2018</strong>: &ensp;&ensp; <a href="https://www.cs.ox.ac.uk/teaching/courses/2018-2019/ml/index.html"> Machine Learning</a> (University of Oxford). </p>

             <p> <strong>Michaelmas Term, 2017</strong>: &ensp;&ensp; <a href="https://www.cs.ox.ac.uk/teaching/courses/2017-2018/ml/index.html"> Machine Learning</a> (University of Oxford). </p>

             <p> <strong>Second Semester, 2014</strong>: &ensp;&ensp; <a href="#"> C++ Programming</a> (The University of Hong Kong). </p>

            </td>
            </tr></tbody>
    </table>


    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Mentoring</heading>
            <p> <strong>Alexander Trevithick</strong> (Oct 2019 - ): &ensp;&ensp; Exeter College at University of Oxford.</p>
              <p> <strong><a href="https://www.cs.ox.ac.uk/people/qingyong.hu/">Qingyong Hu</a></strong> (Oct 2018 - ): &ensp;&ensp; Department of Computer Science at University of Oxford.</p>
              <p> <strong><a href="https://scholar.google.com/citations?user=lj-xw5cAAAAJ&hl=en">Jianan Wang</a></strong> (May - Dec 2018): &ensp;&ensp; Now with <a href="https://deepmind.com/">Google DeepMind</a>. </p>
              <p> <strong><a href="https://uk.linkedin.com/in/zlai">Zihang Lai</a></strong> (Oct 2017 - Mar 2018): &ensp;&ensp; Now with <a href="https://www.robots.ox.ac.uk/~vgg/">VGG</a>. </p>
          </td>
       </tr></tbody>
    </table>


    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like playing tennis on <a href="./imgs/tennis_lawn.jpg">lawns</a>,
               <a href="./imgs/tennis_clay.jpg">clays</a>, and <a href="./imgs/tennis_hard.jpg">hard surfaces</a>.
               I also like to fly drones for landscape photography. Here's a video over the historic Oxford
               [<a href="https://www.youtube.com/watch?v=uIQpZl4kdP4&t=1s">Youtube</a>,
               <a href="https://v.qq.com/x/page/c0339p9vnht.html">腾讯视频</a>], and another video for the scenic Lake District
               [<a href="https://www.youtube.com/watch?v=v-hYnwAIkSE">Youtube</a>]. Remember to turn up the volume for the background music.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
    <script type="text/javascript" id="clstr_globe"  src="//cdn.clustrmaps.com/globe.js?d=1yPWWuOlXo22MrO9sRinBO9GUjLHe88Yk0lOK35nmQA"></script>
    </p></td>
    </tr>
    </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
               <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
		       <p align="right"><font size="2"> Last update: 2020.09.12. <a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>

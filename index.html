<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>Bo Yang | The Hong Kong Polytechnic University</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/polyu_icon.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>


    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center"><name>Bo Yang</name></p>
                  <p align="justify">I am an Assistant Professor (2020.11-) in the <a href="https://www.comp.polyu.edu.hk/">Department of Computing</a>
                    at <a href="https://www.polyu.edu.hk/">The Hong Kong Polytechnic University</a>. I completed my D.Phil degree (2016.10-2020.09)
                    in the <a href="http://www.cs.ox.ac.uk/">Department of Computer Science</a>
                    at <a href="http://www.ox.ac.uk/">University of Oxford</a>,
                    supervised by Profs. <a href="https://www.cs.ox.ac.uk/people/niki.trigoni/">Niki Trigoni</a>
                    and <a href="https://www.cs.ox.ac.uk/people/andrew.markham/">Andrew Markham</a>.
                    Prior to Oxford, I obtained an M.Phil degree from <a href="https://www.hku.hk/">The University of Hong Kong</a> and a B.Eng degree from <a href="https://english.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>.

                    </br></br>
                    In my D.Phil study, I interned at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).
                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.
                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).

	            </br>
                </p><p align="center">
                    <a href="mailto:bo.yang[-at-]polyu.edu.hk">Email</a> /
                    <a href="https://vlar-group.github.io/">vLAR Group</a> /
                    <a href="https://scholar.google.com/citations?hl=en&user=VqUAqz8AAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> /
                    <a href="https://github.com/Yang7879">Github</a>
                </p>
              </td>
			  <td align="right"> <img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;"></td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
              <p align="justify">I lead the <a href="https://vlar-group.github.io/"> Visual Learning and Reasoning (vLAR) Group</a>,
               focusing on the fundamental research problems in machine learning, computer vision, and robotics.
               Our research goal is to build intelligent systems which endow machines to recover, understand, and eventually interact with the real 3D world.
               This includes accurate and efficient recognition, segmentation and reconstruction of all individual objects within large-scale 3D scenes.
		   </p></br>
		   <font color="red"><strong>Openings (Oct 2022 - ):</strong> </font></br></br>

            <font color="red">Several fully funded PostDoc/PhD/RA positions are available now. Email me with your CV and transcripts!
                <p>
                (All emails/CVs are carefully read and evaluated. Only matched candidates will be responded.) </font>
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>News & Activities</heading>
            <p style="font-size:13px"> <strong>[2022.09.03]</strong> Our papers <a href="https://arxiv.org/abs/2210.04458">OGC</a> and <a href="https://arxiv.org/abs/2210.02324">UnsupObjSeg</a> are accepted by NeurIPS 2022.</p>

            <p style="font-size:13px"> <strong>[2022.08.15]</strong> Our paper <a href="https://arxiv.org/abs/2208.07227">DM-NeRF</a> is on arXiv.</p>

            <p style="font-size:13px"> <strong>[2022.07.03]</strong> Our paper <a href="https://arxiv.org/abs/2104.04891">SQN</a> is accepted by ECCV 2022.</p>

            <p style="font-size:13px"> <strong>[2022.05.26]</strong> Our <a href="https://ieeexplore.ieee.org/abstract/document/9792207">extension</a> of SpinNet is accepted by TPAMI.</p>

            <p style="font-size:13px"> <strong>[2022.05.04]</strong> We are going to organize <a href="https://urban3dchallenge.github.io/">The 2nd Challenge on Point Cloud Understanding</a> at ECCV 2022.</p>

            <p style="font-size:13px"> <strong>[2022.04.19]</strong> Our paper <a href="https://arxiv.org/abs/2204.09138">RangeUDF</a> is on arXiv.</p>

            <p style="font-size:13px"> <strong>[2021.11.11]</strong> Our <a href="https://arxiv.org/abs/2009.03137">extension</a> of SensatUrban is accepted by IJCV.</p>

            <p style="font-size:13px"> <strong>[2021.07.23]</strong> Our paper <a href="http://arxiv.org/abs/2010.04595">GRF</a> for neural rendering is accepted by ICCV 2021.</p>

            <p style="font-size:13px"> <strong>[2021.05.15]</strong> Our <a href="https://arxiv.org/abs/2107.02389">extension</a> of RandLA-Net is accepted by TPAMI.</p>

            <p style="font-size:13px"> <strong>[2021.04.09]</strong> We are going to organize <a href="https://urban3dchallenge.github.io/2021/">The 1st Challenge on Point Cloud Understanding</a> at ICCV 2021.</p>

            <p style="font-size:13px"> <strong>[2021.03.01]</strong> Our papers <a href="https://arxiv.org/abs/2009.03137">SensatUrban</a> and <a href="https://arxiv.org/abs/2011.12149">SpinNet</a> are accepted by CVPR 2021.</p>

            <p style="font-size:13px"> <strong>[2021.02.28]</strong> Our paper <a href="https://arxiv.org/abs/2103.11562">RadarLoc</a> is accepted by ICRA 2021.</p>

            <p style="font-size:13px"> <strong>[2020.11.28]</strong> We organize a tutorial about <a href="http://3dlearning.cn/">3D point cloud learning</a> at 3DV 2020.</p>

            <p style="font-size:13px"> <strong>[2020.02.27]</strong> Our paper <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a> for 3D semantic segmentation is accepted by CVPR 2020. </p>

            <p style="font-size:13px"> <strong>[2019.09.03]</strong> Our paper <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> is accepted as a spotlight at NeurIPS 2019. </p>
              <p style="font-size:13px"> <strong>[2019.08.16]</strong> Our paper <a href="https://link.springer.com/journal/11263">AttSets</a> is accepted by IJCV. </p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading> Five Selected Recent Publications <a href="https://vlar-group.github.io/paper.html"> (Full list at vLAR research page) </a> </heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>

        <td width="20%"><img src="./imgs/22_neurips_ogc.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/2210.04458">
	             <papertitle>OGC: Unsupervised 3D Object Segmentation from Rigid Dynamics of Point Clouds</papertitle></a>
                 <br>Z. Song, <strong>B. Yang</strong>
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2210.04458">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=dZBjvKWJ4K0"><font color="red">Video</font></a>/
                 <a href="https://github.com/vLAR-group/OGC"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=OGC&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We introduce the first unsupervised 3D object segmentation method on point clouds.</p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/22_neurips_unsupobjseg.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/2210.02324">
	             <papertitle>Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images</papertitle></a>
                 <br>Y. Yang, <strong>B. Yang</strong>
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2210.02324">arXiv</a> /
                 <a href="https://vlar-group.github.io/UnsupObjSeg.html"><font color="red">Project Page</font></a>/
                 <a href="https://github.com/vLAR-group/UnsupObjSeg"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=UnsupObjSeg&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We systematically investigate the effectiveness of existing unsupervised models on challenging real-world images.</p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/21_iccv_grf.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://arxiv.org/abs/2010.04595">
                 <papertitle>GRF: Learning a General Radiance Field for 3D Representation and Rendering</papertitle></a>
                 <br>A. Trevithick, <strong>B. Yang</strong> <br>
                 <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021
                 <br>
                 <a href="http://arxiv.org/abs/2010.04595">arXiv</a> /
                 <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <p align="justify" style="font-size:13px">We introduce a simple implicit neural function to represent complex 3D geometries purely from 2D images.
                 </p>
                <p></p>
            </td>
        </tr>


        <td width="20%"><img src="./imgs/21_tpami_randla.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2107.02389">
                 <papertitle>Learning Semantic Segmentation of Large-Scale Point Clouds with Random Sampling</papertitle></a>
                 <br>Q. Hu, <strong>B. Yang*</strong>, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham<br>
                 <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2021 <font color="red"><strong>(IF=16.39)</strong></font><br>
                 <a href="https://arxiv.org/abs/2107.02389">arXiv</a>/
                 <a href="https://ieeexplore.ieee.org/document/9440696">IEEE Xplore</a>/
                 <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                 <br>(* indicates corresponding author)
                 </p><p></p>
                  <p align="justify" style="font-size:13px">The journal version of our RandLA-Net. More experiments and analysis are included.</p>
                <p></p>
            </td>
        </tr>


        <!--
        <td width="20%"><img src="./imgs/21_arXiv_sqn.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2104.04891">
                 <papertitle>SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point Clouds with 1000x Fewer Labels</papertitle></a>
                 <br>Q. Hu, <strong>B. Yang*</strong>, G. Fang, Y. Guo, A. Leonardis, N. Trigoni, A. Markham<br>
                 <em>arXiv</em>, 2021
                 <br>
                 <a href="https://arxiv.org/abs/2104.04891">arXiv</a> /
                 <a href="https://github.com/QingyongHu/SQN"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SQN&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <br>(* indicates corresponding author)
                 <p align="justify" style="font-size:13px">We introduce a simple weakly-supervised neural network to learn precise 3D semantics for large-scale point clouds.
                 </p>
                <p></p>
            </td>
        </tr>
        -->


        <!--
        <td width="20%"><img src="./imgs/21_cvpr_spinnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2011.12149">
                 <papertitle>SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration</papertitle></a>
                 <br>S. Ao*, Q. Hu*, <strong>B. Yang</strong>, A. Markham, Y. Guo<br>
                 <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                 <br>
                 <a href="https://arxiv.org/abs/2011.12149">arXiv</a> /
                 <a href="https://github.com/QingyongHu/SpinNet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SpinNet&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <br>(* indicates equal contributions)
                 <p align="justify" style="font-size:13px">We introduce a simple and general neural network to register pieces of 3D point clouds.
                 </p>
                <p></p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/21_cvpr_sensaturban.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://arxiv.org/abs/2009.03137">
                 <papertitle>Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset, Benchmarks and Challenges</papertitle></a>
                 <br>Q. Hu, <strong>B. Yang*</strong>, S. Khalid, W. Xiao, N. Trigoni, A. Markham<br>
                 <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                 <br>
                 <a href="http://arxiv.org/abs/2009.03137">arXiv</a> /
                 <a href="https://www.youtube.com/watch?v=IG0tTdqB3L8">Demo</a> /
                 <a href="https://github.com/QingyongHu/SensatUrban"><font color="red">Project page</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=SensatUrban&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                <br>(* indicates corresponding author)
                <p align="justify" style="font-size:13px">We introduce an urban-scale photogrammetric point cloud dataset
                            and extensively evaluate and analyze the state-of-the-art algorithms on the dataset.
                  </p>
                <p></p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/21_icra_radarloc.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="#">
                 <papertitle>RadarLoc: Learning to Relocalize in FMCW Radar</papertitle></a>
                 <br>W. Wang, P.P.B. de Gusmao, <strong>B. Yang</strong>, A. Markham, N. Trigoni<br>
                 <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2021
                 <br>
                 <a href="#">arXiv</a>
                 <p align="justify" style="font-size:13px">We introduce a simple end-to-end neural network with self-attention to estimate global poses from FMCW radar scans.
                 </p>
                <p></p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/20_arXiv_pointloc.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2003.02392">
	            <papertitle>PointLoc: Deep Pose Regressor for LiDAR Point Cloud Localization</papertitle></a>
                <br>Wei Wang, Bing Wang, Peijun Zhao, Changhao Chen, Ronald Clark, <strong>B. Yang</strong>, Andrew Markham, Niki Trigoni
                <br>
                <a href="https://arxiv.org/abs/2003.02392">arXiv, 2020</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We present a learning-based LiDAR relocalization framework to efficiently estimate 6-DoF poses from LiDAR point clouds. </p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/20_cvpr_randlanet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1911.11236">
	            <papertitle>RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</papertitle></a>
                <br>Q. Hu, <strong>B. Yang*</strong>, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1911.11236">arXiv</a> /
                <a href="http://www.semantic3d.net/view_results.php">Semantic3D Benchmark</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/k_oROm1Zr6l0YNKGELx3Bw"><font color="red">(新智元,</font></a>
                <a href="https://mp.weixin.qq.com/s/Ed9v6I6l2tLTHmMW7B3O3g"><font color="red">AI科技评论,</font></a>
                <a href="https://mp.weixin.qq.com/s/TTv6pSPjmdsEF4kvVY-ZzQ"><font color="red">CVer)</font>/</a>
                <a href="https://www.youtube.com/watch?v=Ar3eY_lwzMk"><font color="red">Video</font></a>/
                <a href="https://github.com/QingyongHu/RandLA-Net"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=QingyongHu&repo=RandLA-Net&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>(* indicates corresponding author)
                </p><p></p>
			    <p align="justify" style="font-size:13px">We introduce an efficient and lightweight neural architecture to directly infer per-point semantics for large-scale point clouds. </p>
            </td>
        </tr>
        -->

        <td width="20%"><img src="./imgs/19_neurips_3d_bonet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/1906.01140">
	             <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</papertitle></a>
                 <br><strong>B. Yang</strong>, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, N. Trigoni
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="red"><strong>(Spotlight, 200/6743)</strong></font>
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/1906.01140">arXiv</a> /
                 <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> /
                 <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> /
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font color="red">(新智元,</font></a>
                 <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a>
                 <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font color="red">泡泡机器人)</font>/</a>
                 <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao"><font color="red">Video</font></a>/
                 <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds.
                  It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                <p></p>
            </td>
        </tr>


        <!--
        <td width="20%"><img src="./imgs/19_iros_deeppco.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1910.11088">
	            <papertitle>DeepPCO: End-to-End Point Cloud Odometry through Deep Parallel Neural Network</papertitle></a>
                <br>W. Wang,  M.R.U. Saputra, P. Zhao, P. Gusmao, <strong>B. Yang</strong>, C. Chen, A. Markham, N. Trigoni<br>
		        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2019 <br>
                <a href="https://arxiv.org/abs/1910.11088">arXiv</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We propose a novel end-to-end deep parallel neural network to estimate the 6-DOF poses using consecutive 3D point clouds.</p>
            </td>
        </tr>
        -->

        <!--
	    <td width="20%"><img src="./imgs/19_ijcv_attsets.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">
	            <papertitle>Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction</papertitle></a>
                <br><strong>B. Yang</strong>, S. Wang, A. Markham, N. Trigoni<br>
                <em>International Journal of Computer Vision (IJCV)</em>, 2019 <font color="red"><strong>(IF=6.07)</strong></font><br>
		        <a href="https://arxiv.org/abs/1808.00758">arXiv</a>/
		        <a href="https://link.springer.com/article/10.1007/s11263-019-01217-w">Springer Open Access</a>/
                <a href="https://github.com/Yang7879/AttSets"><font color="red">Code</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=AttSets&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px"> We propose an attentive aggregation module together
                    with a training algorithm for multi-view 3D object reconstruction.
                    It outperforms all existing poolings and recurrent neural networks.</p>
            </td>
        </tr>
        -->

        <!--
	    <td width="20%"><img src="./imgs/19_cvprw_embeddings.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">
	            <papertitle>Learning Semantically Meaningful Embeddings Using Linear Constraints</papertitle></a>
                <br>S. Lin, <strong>B. Yang</strong>, R. Birke, R. Clark<br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)</em>, 2019<br>
                <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/Explainable_AI/Lin_Learning_Semantically_Meaningful_Embeddings_Using_Linear_Constraints_CVPRW_2019_paper.html">CVF Open Access</a>
                </p><p></p>
			    <p align="justify" style="font-size:13px">We propose a simple embedding learning method that jointly optimises for an auto-encoding reconstruction task
                    and for estimating the corresponding attribute labels.</p>
            </td>
        </tr>
        -->

        <!--
	    <td width="20%"><img src="./imgs/18_tpami_3d_recgan++.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1802.00411">
	           <papertitle>Dense 3D Object Reconstruction from a Single Depth View</papertitle></a>
               <br><strong>B. Yang</strong>, S. Rosa, A. Markham, N. Trigoni, H. Wen<br>
               <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2018 <font color="red"><strong>(IF=17.73)</strong></font><br>
               <a href="https://arxiv.org/abs/1802.00411">arXiv</a>/
               <a href="https://ieeexplore.ieee.org/abstract/document/8453803">IEEE Xplore</a>/
               <a href="https://github.com/Yang7879/3D-RecGAN-extended"><font color="red">Code</font></a>
               <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-RecGAN-extended&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a novel neural architecture to reconstruct the complete 3D structure of a given object
                   from a single arbitrary depth view using generative adversarial networks.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/18_ijcai_3d_physnet.gif" alt="this slowpoke moves" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1805.00328">
	           <papertitle>3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations</papertitle></a>
               <br>Z. Wang, S. Rosa, <strong>B. Yang</strong>, S. Wang, N. Trigoni, A. Markham<br>
               <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2018 <br>
               <a href="https://arxiv.org/abs/1805.00328">arXiv</a>/
               <a href="https://github.com/vividda/3D-PhysNet"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=vividda&repo=3D-PhysNet&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a neural framework to predict how a 3D object will deform
                   under an applied force using intuitive physics modelling.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/18_cvprw_3r_d.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w9/html/Yang_Learning_3D_Scene_CVPR_2018_paper.html">
	           <papertitle>Learning 3D Scene Semantics and Structure from a Single Depth Image</papertitle></a>
               <br><strong>B. Yang*</strong>, Z. Lai*, X. Lu, S. Lin, H. Wen, A. Markham, N. Trigoni<br>
               <em>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR-W)</em>, 2018 <br>
               <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w9/html/Yang_Learning_3D_Scene_CVPR_2018_paper.html">CVF Open Access</a> /
               <a href="https://ieeexplore.ieee.org/abstract/document/8575531">IEEE Xplore</a>
               <br>(* indicates equal contribution)
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose an efficient and holistic pipeline to simultaneously learn
                   the semantics and structure of a scene from a single depth image.</p>
            </td>
        </tr>
        -->

         <!--
        <td width="20%"><img src="./imgs/18_icra_defonet.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1804.05928">
	           <papertitle>Defo-Net: Learning Body Deformation Using Generative Adversarial Networks</papertitle></a>
               <br>Z. Wang, S. Rosa, L. Xie, <strong>B. Yang</strong>, S. Wang, N. Trigoni, A. Markham<br>
               <em>IEEE International Conference on Robotics and Automation (ICRA) </em>, 2018 <br>
               <a href="https://arxiv.org/abs/1804.05928">arXiv</a> /
               <a href="https://www.youtube.com/watch?v=noG5DDX3coQ"><font color="red">Video</font></a>/
               <a href="https://ieeexplore.ieee.org/abstract/document/8462832">IEEE Xplore</a>/
               <a href="https://github.com/vividda/Defo-Net"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=vividda&repo=Defo-Net&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a novel generative adversarial network to predict
                   body deformations under external forces from a single RGB-D image.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/17_iccvw_3d_recgan.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://arxiv.org/abs/1708.07969">
	           <papertitle>3D Object Reconstruction from a Single Depth View with Adversarial Learning</papertitle></a>
               <br><strong>B. Yang</strong>, H. Wen, S. Wang, R. Clark, A. Markham, N. Trigoni<br>
               <em>IEEE International Conference on Computer Vision Workshops (ICCV-W) </em>, 2017 <br>
               <a href="https://arxiv.org/abs/1708.07969">arXiv</a> /
               <a href="https://ieeexplore.ieee.org/abstract/document/8265295">IEEE Xplore</a>/
               <a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650730434&idx=4&sn=4a03526f020f30cc65b52976bb56f352&scene=0"><font color="red"> News: 机器之心</font>/</a>
               <a href="https://github.com/Yang7879/3D-RecGAN"><font color="red">Code</font> </a>
               <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-RecGAN&type=star&count=true&size=small"
               frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a novel approach to reconstruct the complete 3D structure of a given
                   object from a single arbitrary depth view using generative adversarial networks.</p>
            </td>
        </tr>
        -->


        <!--
        <td width="20%"><img src="./imgs/16_mswim_bcs.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://dl.acm.org/citation.cfm?id=2989132">
	           <papertitle>Updating Wireless Signal Map with Bayesian Compressive Sensing</papertitle></a>
               <br><strong>B. Yang</strong>, S. He, S-H G. Chan<br>
               <em>ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM) </em>, 2016 <br>
               <a href="https://dl.acm.org/citation.cfm?id=2989132">ACM DL</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose Compressive Signal Reconstruction (CSR), a novel learning system
                   employing Bayesian compressive sensing (BCS) for online signal map update.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/15_compind_3dscan.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0166361515000913">
	           <papertitle>A mechanised 3D scanning method for item-level radio frequency identification of palletised products</papertitle></a>
               <br>S.H. Choi, <strong>B. Yang</strong>, H.H. Cheung<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/abs/pii/S0166361515000913">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose a mechanised 3D scanning method for identification of
                   tagged products in large numbers to facilitate supply chain management.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/15_compind_item_rfid.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/pii/S0166361515000482">
	           <papertitle>Item-level RFID for Enhancement of Customer Shopping Experience in Apparel Retail</papertitle></a>
               <br>S.H. Choi, Y.X. Yang, <strong>B. Yang</strong>, H.H. Cheung<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/pii/S0166361515000482">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We propose an item-level RFID-enabled retail store management system
                   for relatively high-end apparel products to provide customers with more leisure, interaction for product information.</p>
            </td>
        </tr>
        -->

        <!--
        <td width="20%"><img src="./imgs/15_compind_tag_proc.jpg" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	           <p><a href="https://www.sciencedirect.com/science/article/pii/S0166361515000147">
	           <papertitle>RFID Tag Data Processing in Manufacturing for Track-and-Trace Anti-counterfeiting</papertitle></a>
               <br>S.H. Choi, <strong>B. Yang</strong>, H.H. Cheung, Y.X. Yang<br>
               <em>Computers in Industry </em>, 2015  <font color="red"><strong>(IF=4.77)</strong></font> <br>
               <a href="https://www.sciencedirect.com/science/article/pii/S0166361515000147">Elsevier ScienceDirect</a>
               </p><p></p>
               <p align="justify" style="font-size:13px">We present a track-and-trace anti-counterfeiting system,
                   and propose a tag data processing and synchronization algorithm to generate initial e-pedigrees for products.</p>
            </td>
        </tr>
        -->

        </tbody>
    </table>


    <!--Thesis  -->
     <br>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
        <tbody><tr>
          <td><heading>DPhil (PhD) Thesis</heading>
          </tr></tbody>
    </table>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>
        <td width="20%"><img src="./imgs/oxford_logo.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="https://arxiv.org/abs/2010.09582">
                 <papertitle>Learning to Reconstruct and Segment 3D Objects</papertitle></a>
                 <br>
                 <br><strong>B. Yang</strong><br>
                 <a href="https://ora.ox.ac.uk/objects/uuid:5f9cd30d-0ee7-412d-ba49-44f5fd76bf282"> Oxford Research Archive (PDF), 2020; </a>
                 <a href="https://mp.weixin.qq.com/s/-l0pkDt1rgfYbG2K4Vu5cg"><font color="red"> (News: 机器之心报道)</font></a>
                 <br>
                 <br><strong>Thesis committee</strong> (Transfer/Confirmation/Viva):<br>
                 <a href="https://www.cs.ox.ac.uk/people/alessandro.abate/home.html">Alessandro Abate</a>,
                 <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>,
                 <a href="https://mpawankumar.info/">Pawan Kumar</a>,
                 <a href="https://scholar.google.co.uk/citations?user=UZ5wscMAAAAJ&hl=en">Andrew Zisserman</a>.<br>
                 <p align="justify" style="font-size:13px">This thesis aims to understand scenes and the objects within them by learning general and
                robust representations using deep neural networks, trained on large-scale real-world 3D data.
                In particular, the thesis makes three core contributions from object-level 3D shape estimation
                from single or multiple views to scene-level semantic understanding.
                 </p>
                <p></p>
            </td>
        </tr>

        </tbody>
    </table>

    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Talks & Services</heading>
             <p style="font-size:13px"> <strong>[2022.06]</strong> Invited talk about 3D Scene Reconstruction, Decomposition and Manipulation at Xiamen University.</p>
             <p style="font-size:13px"> <strong>[2021.10]</strong> Invited talk about <a href="https://www.bilibili.com/video/BV13f4y1u7QL">3D Representation Learning</a> at GAMES Webinar.</p>
             <p style="font-size:13px"> <strong>[2021.04]</strong> Invited talk about <a href="https://www.bilibili.com/video/BV1164y1v7zK?p=5">Beyond Supervised Learning for 3D Representations</a> at a CSIG workshop.</p>
             <p style="font-size:13px"> <strong>[2020.10]</strong> Invited talk about 3D Scene Understanding at <a href="https://wonderlandai.com/team-member/bo-yang/">Wonderland AI Summit</a>. Check out the <a href="./imgs/wonderlandai.mp4">trailer</a>.</p>
             <p style="font-size:13px"> <strong>[2020.09]</strong> Invited talk about 3D Point Cloud Segmentation at <a href="https://mfi2020.org/workshoptutorials/#wang">MFI 2020</a>.</p>
             <p style="font-size:13px"> <strong>[2020.03]</strong> Invited talk about our <a href="https://arxiv.org/abs/1911.11236">RandLA-Net</a>
                and <a href="https://arxiv.org/abs/1906.01140">3D-BoNet</a> at Shenlan.
                Here are the <a href="https://www.shenlanxueyuan.com/open/course/53">Video</a>
                and <a href="https://www.dropbox.com/s/80w91bqzfdl5vow/%E6%B7%B1%E8%93%9D%E5%AD%A6%E9%99%A2%E5%85%AC%E5%BC%80%E8%AF%BE_20200308.pdf?dl=0">Slides</a>. </p>
             <p style="font-size:13px"> <strong>[2018 -]</strong> Regularly review papers for top-tier conferences and journals in machine learning and computer vision.</p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Teaching</heading>
             <p> <strong>Fall Term, 2022</strong>: &ensp;&ensp; <a href="https://www.comp.polyu.edu.hk/en-us/study/taught-postgraduate-programmes/tpg-subject"> Machine Learning and Data Analytics </a> (The Hong Kong Polytechnic University). </p>
             <p> <strong>Spring Term, 2022</strong>: &ensp;&ensp; <a href="https://www.comp.polyu.edu.hk/en-us/study/taught-postgraduate-programmes/tpg-subject"> Machine Learning and Data Analytics </a> (The Hong Kong Polytechnic University). </p>
             <p> <strong>Fall Term, 2021</strong>: &ensp;&ensp; <a href="https://www.comp.polyu.edu.hk/en-us/study/taught-postgraduate-programmes/tpg-subject"> Machine Learning and Data Analytics </a> (The Hong Kong Polytechnic University). </p>
             <p> <strong>Spring Term, 2021</strong>: &ensp;&ensp; <a href="https://www.comp.polyu.edu.hk/en-us/study/taught-postgraduate-programmes/tpg-subject"> Machine Learning and Data Analytics </a> (The Hong Kong Polytechnic University). </p>
             <p> <strong>Hilary Term, 2019</strong>: &ensp;&ensp; <a href="https://www.cs.ox.ac.uk/teaching/courses/2018-2019/KRR/"> Knowledge Representation & Reasoning </a> (University of Oxford). </p>
             <p> <strong>Michaelmas Term, 2018 & 2017</strong>: &ensp;&ensp; <a href="https://www.cs.ox.ac.uk/teaching/courses/2018-2019/ml/index.html"> Machine Learning</a> (University of Oxford). </p>
             <p> <strong>Spring Term, 2014</strong>: &ensp;&ensp; <a href="#"> C++ Programming</a> (The University of Hong Kong). </p>
            </td>
            </tr></tbody>
    </table>


    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Mentoring <a href="https://vlar-group.github.io/people.html"> (Full list at vLAR member page)</a> </heading>
              <p> <strong><a href="https://www.cs.ox.ac.uk/people/qingyong.hu/">Qingyong Hu</a></strong> (Oct 2018 - ): &ensp;&ensp; Department of Computer Science at University of Oxford.</p>
              <p> <strong><a href="https://alextrevithick.github.io/">Alexander Trevithick</a></strong> (Oct 2019 - Mar 2021): &ensp;&ensp; Now PhD at UCSD.</p>
              <p> <strong><a href="https://scholar.google.com/citations?user=lj-xw5cAAAAJ&hl=en">Jianan Wang</a></strong> (May - Dec 2018): &ensp;&ensp; Now with <a href="https://deepmind.com/">Google DeepMind</a>. </p>
              <p> <strong><a href="https://uk.linkedin.com/in/zlai">Zihang Lai</a></strong> (Oct 2017 - Mar 2018): &ensp;&ensp; Now PhD at <a href="https://www.ri.cmu.edu/ri-people/zihang-lai/">CMU</a>. </p>
          </td>
       </tr></tbody>
    </table>


    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like playing tennis on <a href="./imgs/tennis_lawn.jpg">lawns</a>,
               <a href="./imgs/tennis_clay.jpg">clays</a>, and <a href="./imgs/tennis_hard.jpg">hard surfaces</a>.
               I also like to fly drones for landscape photography. Here's a video over the historic Oxford
               [<a href="https://www.youtube.com/watch?v=uIQpZl4kdP4&t=1s">Youtube</a>,
               <a href="https://v.qq.com/x/page/c0339p9vnht.html">腾讯视频</a>], and another video for the scenic Lake District
               [<a href="https://www.youtube.com/watch?v=v-hYnwAIkSE">Youtube</a>]. Remember to turn up the volume for the background music.
		   <!--</br></br>-->
		   <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
    <script type="text/javascript" id="clstr_globe"  src="//cdn.clustrmaps.com/globe.js?d=1yPWWuOlXo22MrO9sRinBO9GUjLHe88Yk0lOK35nmQA"></script>
    </p></td>
    </tr>
    </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
               <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
		       <p align="right"><font size="2"> Last update: 2022.10. <a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
